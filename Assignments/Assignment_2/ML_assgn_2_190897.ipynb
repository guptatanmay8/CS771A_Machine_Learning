{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML_assgn_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gde8znA82rAe"
      },
      "source": [
        "import numpy as np\n",
        "import time as t"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBi1IQlq1VzO"
      },
      "source": [
        "Q1 Gradient Descent algorithm :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0awWHL98zt0g"
      },
      "source": [
        "def gradient_descent(gradient,init_,learn_rate, n_iter=50, tol=1e-06):\n",
        "    x = init_\n",
        "    for _ in range(n_iter):\n",
        "        delta = -learn_rate*gradient(x)\n",
        "        if np.all(np.abs(delta) <= tol):\n",
        "            break\n",
        "        x += deltastochastic\n",
        "    return round(x*1000)/1000\n",
        "\n",
        "#this function is given in the question itself"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKy6CxMnWi_e"
      },
      "source": [
        "(a) Use this function to find minima for (i) $x^2 + 3x+4$ and (ii) $x^4 â€“ 3x^2 +2x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wldN_nH4XDok"
      },
      "source": [
        "**Solution**: To find minima for a polynomial , we will use the above gradient descent algorithm with gradient at any x be its derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pZCHOXc1hzY"
      },
      "source": [
        "(i) For $x^2 + 3x + 4$ , we will use learn_rate = 0.1 , initial value of $x$ = 0 and gradient at any x be $2x + 3$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF5caqgm1_EK",
        "outputId": "3970be6b-7717-4d78-ffd7-31641943242a"
      },
      "source": [
        "#here we are passing gradient as an inline function which is the derivative of the function given in the question and we are finding the minima using it\n",
        "min1 = gradient_descent(gradient=lambda x: 2*x + 3, init_= 0, learn_rate=0.2)\n",
        "print(\"Minimum value at x = \",min1)\n",
        "print(\"Minimum = \", min1**2 + 3*min1 + 4)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value at x =  -1.5\n",
            "Minimum =  1.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrPyYUoAGFv"
      },
      "source": [
        "\n",
        "(ii) For $x^4 - 3x^2 + 2x$ , we will use learn_rate = 0.1 , initial value of $x$ = 0 and gradient at any x be $4x^3 - 6x + 2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Yd5ZTH2TYR",
        "outputId": "6438c76b-e8ce-44c3-e7d6-ee1bde441e75"
      },
      "source": [
        "#We have done the same thing here also\n",
        "min_2 = gradient_descent(gradient= lambda x: 4*x**3 - 6*x + 2, init_= 0, learn_rate=0.1)\n",
        "print(\"Minimum value will be at x = \", min_2)\n",
        "print(\"Minimum value = \", min_2**4 - 3*min_2**2 + 2*min_2)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value will be at x =  -1.366\n",
            "Minimum value =  -4.848076206064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUKQi8ikp963"
      },
      "source": [
        "(b) For linear regression : y  = ax + b , let's take mean squared error to calculate cost function.\n",
        "\n",
        "Cost Function = $\\frac{1}{n}\\sum_{i=1}^{n} (y_i - y_{ipred})^2$\n",
        ". Here $y_{ipred} = ax_i + b$ (theoretically). In gradient descent algorithm, we need to find the gradients at every x in order to update the value of x. In order to calculate the gradient , we need to find the partial derivative of the cost function wrt to a as well as wrt to b.\n",
        "\n",
        "$D_a = \\frac{ \\partial ( Cost Function ) }{\\partial a} = \\frac{ \\partial (  \\frac{1}{n}\\sum_{i=1}^{n} (y_i - y_{ipred})^2 ) }{\\partial a} = \\frac{-2}{n} \\sum_{i=1}^{n} x_i(y_i - y_{ipred})$\n",
        "\n",
        "$D_b = \\frac{ \\partial ( Cost Function ) }{\\partial b} = \\frac{ \\partial (  \\frac{1}{n}\\sum_{i=1}^{n} (y_i - y_{ipred})^2 ) }{\\partial b} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - y_{ipred})$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jREcua-R7Dy"
      },
      "source": [
        "# Gradient Function where we have put in the above calculated gradient function in the code\n",
        "def gradient(x, X_here = X, Y_here = y):\n",
        "    (a,b) = x\n",
        "    N = float(len(Y_here))\n",
        "    y_curr = (a * X_here) + b\n",
        "    grad_a = -(2/N) * sum(X_here * (Y_here - y_curr))  #this gives partial derivative wrt a\n",
        "    grad_b = -(2/N) * sum(Y_here - y_curr)  ##this gives partial derivative wrt b\n",
        "    return np.array([grad_a,grad_b])\n",
        "\n",
        "#Here X_here and Y_here are training examples"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZuiemTAKunZ"
      },
      "source": [
        "(c) Generating artificial data and applying gradient descent algorithm to find optimum parameters {a,b}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poF_BWQbLTs4"
      },
      "source": [
        "(i) Generating random training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2wtHWTkLavA"
      },
      "source": [
        "#given in the question\n",
        "np.random.seed(0)\n",
        "X = 2.5 * np.random.randn(10000) + 1.5\n",
        "res = 1.5 * np.random.randn(10000)\n",
        "y = 2 + 0.3* X + res \n",
        "#X,y forms the random dataset"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-dHZbSCGDAU"
      },
      "source": [
        "def gradient_descent(gradient,init_,learn_rate, n_iter=50, tol=1e-06):\n",
        "    x = init_\n",
        "    for _ in range(n_iter):\n",
        "        delta = -learn_rate * gradient(x)\n",
        "        if np.all(np.abs(delta) <= tol):\n",
        "            break\n",
        "        x += delta\n",
        "    return np.round(x*1000)/1000\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIPRnhQ9MCac"
      },
      "source": [
        "Running the linear regression y = ax + b and applying Gradient Descent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Art1cIDWMLiW",
        "outputId": "ae1703e9-f443-489f-8025-e3bffe396762"
      },
      "source": [
        "#running gradient descent function to find the optimal value of x\n",
        "gradient_descent(gradient, init_= (0,0), learn_rate=0.001,n_iter = 10000)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.295, 2.023])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGHfbKdHSc05"
      },
      "source": [
        "(d) Minibatch Stochastic Gradient Descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j77VIsMKS9qm"
      },
      "source": [
        "#minibatch sgd function is deined here\n",
        "def minibatch_sgd(gradient, init_, learn_rate, n_iter = 50, tol = 1e-06, batch_size = 125):\n",
        "  x = init_\n",
        "  for _ in range(n_iter):\n",
        "    indexes = np.random.choice(np.arange(len(X)) , batch_size, replace = False)       #we have taken random indexes which will make our batches\n",
        "    X_batch = X[indexes]    # X value of those indexes\n",
        "    y_batch = y[indexes]    # y value of those indexes\n",
        "    delta = -learn_rate * gradient(x, X_batch, y_batch)   #calculated delta in the similar way\n",
        "    if np.all(np.abs(delta) <= tol):\n",
        "      break\n",
        "    x+= delta\n",
        "\n",
        "  return np.round(x*1000)/1000"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO9esYsclY5l",
        "outputId": "299e7610-9923-44a8-96fa-4c9532b79f43"
      },
      "source": [
        "minibatch_sgd(gradient,init_ = (0,0) , learn_rate= 0.001, n_iter = 10000)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.297, 2.021])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRJQNfbft6go"
      },
      "source": [
        "(e) We will first compare time for both gradient descent and minibatch sgd:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "492h64RmnAwg",
        "outputId": "4436e7a6-b045-4385-c9c5-2e55f2a79a3f"
      },
      "source": [
        "#time taken by GD:\n",
        "tic_gd = t.process_time()   #initial time\n",
        "gradient_descent(gradient, init_= (0,0), learn_rate=0.001,n_iter = 10000)\n",
        "toc_gd = t.process_time()   #Final time\n",
        "\n",
        "time_taken_gd = toc_gd - tic_gd\n",
        "print(\"Time taken by gradient descent is: \", time_taken_gd)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by gradient descent is:  18.605123625000033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V8J6oaJnBZU",
        "outputId": "92d4920c-c5e5-4f45-8f2d-0a609a8a9c90"
      },
      "source": [
        "#time taken by minibatch stochastic gradient descent:\n",
        "tic_sgd = t.process_time()    #initial time\n",
        "minibatch_sgd(gradient, init_= (0,0), learn_rate=0.001,n_iter = 10000)\n",
        "toc_sgd = t.process_time()    #Final time\n",
        "\n",
        "time_taken_sgd = toc_sgd - tic_sgd\n",
        "print(\"Time taken by minibatch stochastic gradient descent is: \", time_taken_sgd)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by minibatch stochastic gradient descent is:  2.9779027830000473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfoHtOPJ2IVE"
      },
      "source": [
        "Therefore, time taken by minibatch stochastic gradient descent is much lower than normal gradient descent, SGD took approx **3** secs for 10,000 iterations, whereas GD took **18.6** secs for the same 10,000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiaNRuW60Z5c",
        "outputId": "1ca34a3e-507d-431f-a4aa-0e7e266e44cd"
      },
      "source": [
        "min_time = 1e9\n",
        "optimal_batch_size = 0\n",
        "for batch_s in range(25,1000,25):   #loop for batch sizes in multiple of 25\n",
        "  tic_sgd = t.process_time()\n",
        "  minibatch_sgd(gradient, init_= (0,0), learn_rate=0.001,n_iter = 10000, batch_size = batch_s)\n",
        "  toc_sgd = t.process_time()\n",
        "\n",
        "  time_taken_sgd = toc_sgd - tic_sgd    #time for each batch size \n",
        "  if(time_taken_sgd < min_time):      #if this time is less than min_time then\n",
        "    min_time = time_taken_sgd         #update value of min_time\n",
        "    optimal_batch_size = batch_s      #update optimal_batch size\n",
        "  \n",
        "  # print(batch_s)\n",
        "\n",
        "print(min_time)\n",
        "print(optimal_batch_size)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5451389610000206\n",
            "225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp07PKN92X57"
      },
      "source": [
        "Therefore, opmitimal batch size that runs minibatch stochastic gradient descent in minimum time is: **225** and the minimum time used for the same is **2.545** secs for 10,000 iterations if we take batch sizes as multiples of 25, we cn calculate the exact optimal batch size by taking increase by 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXuVSsXlnG4Z"
      },
      "source": [
        "##Q2. Finding probability of:\n",
        "(i) Someone having both cold and a fever\n",
        "\n",
        "(ii) Someone who has a cough has a cold.\n",
        "\n",
        "##Solution:-\n",
        "(i) $P(\\text{cold} \\cap \\text{fever}) = P(\\text{fever}/\\text{cold}) \\times P(\\text{cold})=0.307*0.02=0.00614$\n",
        "\n",
        "(ii)\n",
        "\\begin{align*}\n",
        "P(\\text{cold}/\\text{cough}) &= \\frac{P(\\text{cold } \\cap \\text{ cough})}{P(\\text{cough})}\\\\ \n",
        " &= \\frac{P(\\text{cough}/\\text{cold})\\cdot P(\\text{cold})}{P(\\text{cough}/\\text{cold})\\cdot P(\\text{cold})+P(\\text{cough}/\\text{no cold})\\cdot P(\\text{no cold})} \\dots \\textbf{ Eqn. 1}\\\\ \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "P(\\text{lung disease}) &= P(\\text{lung disease/smoke})\\cdot P(\\text{smoke}) + P(\\text{lung disease/no smoke})\\cdot P(\\text{no smoke}) \\\\ &=0.1009*0.2+0.001*0.8 \\\\ &= 0.02098 \\\\ \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "P(\\text{no lung disease}) &= 1 - P(\\text{lung disease})  \\\\ &= 1 - 0.02098 \\\\ &= 0.97902 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "P(cough/cold) &= P(cough/cold/lung disease) \\cdot P(lung disease) + P(cough/cold/no lung disease) \\cdot P(no lung disease)\\\\ &= 0.7525*0.02098+0.505*0.97902 \\\\ &= 0.01578745+0.4944051 \\\\ &= 0.51019255 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "P(cough/nocold) &= P(cough/nocold/lung disease) \\cdot P(lung disease) + P(cough/nocold/no lung disease) \\cdot P(no lung disease)\\\\ &= 0.505*0.02098+0.01*0.97902 \\\\ &= 0.0105949+0.490097902 \\\\ &= 0.0203851 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Putting these values in Eqn $1$, we get\n",
        "\\begin{align*}\n",
        "P(cold/cough) &=  \\frac{0.51019255*0.02}{0.51019255*0.02+0.0203851*0.98}\\\\ &=  \\frac{0.010203851}{0.010203851 + 0.19977398} = \\frac{0.010203851}{0.030181249}\\\\ \\\\ &= 0.338085776\n",
        "\\end{align*}\n",
        "\n",
        "Hence, Required answer is $0.338$ aprroximately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6porZuUyZv"
      },
      "source": [
        "#Q3. \n",
        "MLE for a multinomial Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKDBo1sqvbVx"
      },
      "source": [
        "##Solution:\n",
        "\n",
        "$P(X_1 = x_1,X_2 = x_2,\\dots,X_k = x_k) = \\frac{n!}{x_1! x_2!\\dots x_k!} p_1^{x_1},p_2^{x_2} \\dots,p_k^{x_k},  \\qquad{0\\leq x_i \\leq n} \\quad(\\text{Eqn } 1)$ \n",
        "\n",
        "where $\\sum_{i=1}^k x_i = n$\n",
        "\n",
        "And since all the k outcomes are mutually exclusive, therefore $\\sum_{i=1}^k p_i = 1$ (Eqn $2$)\n",
        "\n",
        "The log-likelihood for the probability mass function -\n",
        "$$log(P(x_1,x_2,\\dots,x_k)) = log (n!) - \\sum_{i=1}^k log (x_i!) + \\sum_{i=1}^k x_i log (p_i!)$$\n",
        "\n",
        "Now, maximizing the log-likelihood function -\n",
        "$$L(p) = log (n!) - \\sum_{i=1}^k log (x_i!) + \\sum_{i=1}^k x_i log (p_i!) + \\lambda (1 - \\sum_{i=1}^k p_i)$$\n",
        "\n",
        "\n",
        "Differentiating $L(p)$ wrt $p_i$, we get\n",
        "$$\\frac{\\partial L(p)}{\\partial p_i} = \\frac{x_i}{p_i} - \\lambda$$\n",
        "Setting this to $0$ gives:\n",
        "$$\\frac{x_i}{p_i} = \\lambda$$\n",
        "\n",
        "\n",
        "$$\\implies p_i = \\frac{x_i}{\\lambda} \\qquad{\\dots \\dots \\text{Eqn $3$}}$$\n",
        "\n",
        "Now, we know from Eqn. 2 that $\\sum_{i=1}^k p_i = 1$, therefore $\\sum_{i=1}^k \\frac{x_i}{\\lambda} = 1$\n",
        "\n",
        "$$ \\sum_{i=1}^k x_i = \\lambda$$\n",
        "\n",
        "Therefore, \n",
        "\n",
        "$$\\lambda = n$$\n",
        "\n",
        "Hence, Eqn $3$ becomes\n",
        "$$p_i = \\frac{x_i}{n}$$ \n",
        "which is the MLE for the parameters of a $k$-sided multinomial distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1kMI59-_gJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}